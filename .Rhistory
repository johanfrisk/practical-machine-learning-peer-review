setwd("~/Documents/CourseraMOOC/R_programming/machine-learning/practical-machine-learning-peer-review")
# reading in the data ---------------------------------------------------------
train <- read.csv(./data/pml-training.csv,
na.strings = "NA",
strip.white = TRUE)
# reading in the data ---------------------------------------------------------
train <- read.csv("./data/pml-training.csv",
na.strings = "NA",
strip.white = TRUE)
# reading in the data ---------------------------------------------------------
train <- tbl_df(read.csv("./data/pml-training.csv",
na.strings = "NA",
strip.white = TRUE))
# reading in the data ---------------------------------------------------------
train <- read.csv("./data/pml-training.csv",
na.strings = "NA",
strip.white = TRUE)
train <- tbl_df(train)
# reading in the data ---------------------------------------------------------
train <- read.csv("./data/pml-training.csv",
na.strings = "NA",
strip.white = TRUE)
train <- tbl_df(train)
eval <- read.csv("./data/pml-testing.csv",
na.strings = "NA",
strip.white = TRUE)
eval <- tbl_df(eval)
# loading required libraries --------------------------------------------------
library(dplyr)
library(caret)
# splitting the training set into train and test ------------------------------
set.seed(123)
inTrain <- createDataPartition(y=train$classe, p=0.9, list=FALSE)
training <- pml.training[inTrain,]
testing <- pml.training[-inTrain,]
# splitting the training set into train and test ------------------------------
set.seed(123)
inTrain <- createDataPartition(y=train$classe, p=0.9, list=FALSE)
training <- train[inTrain,]
testing <- train[-inTrain,]
# fit a boosted tree model via the gbm package --------------------------------
gbmFit <- train(Class ~ ., data = training,
method = "gbm",
trControl = fitControl,
verbose = FALSE)
gbmFit
# fit a boosted tree model via the gbm package --------------------------------
gbmFit <- train(classe ~ user_name +
pitch_arm +
yaw_arm +
roll_arm +
roll_belt +
pitch_belt +
yaw_belt +
gyros_belt_x +
gyros_belt_y +
gyros_belt_z +
accel_belt_x +
accel_belt_y +
accel_belt_z +
magnet_belt_x +
magnet_belt_y +
magnet_belt_z +
gyros_arm_x +
gyros_arm_y +
gyros_arm_z +
accel_arm_x +
accel_arm_y +
accel_arm_z +
magnet_arm_x +
magnet_arm_y +
magnet_arm_z +
roll_dumbbell +
pitch_dumbbell +
yaw_dumbbell,
method="gbm",
trControl = fitControl, # an option for gbm
data=training,
verbose=FALSE)
# loading required libraries --------------------------------------------------
library(dplyr)
library(caret)
library(gbm)
# fit a boosted tree model via the gbm package --------------------------------
gbmFit <- train(classe ~ user_name +
pitch_arm +
yaw_arm +
roll_arm +
roll_belt +
pitch_belt +
yaw_belt +
gyros_belt_x +
gyros_belt_y +
gyros_belt_z +
accel_belt_x +
accel_belt_y +
accel_belt_z +
magnet_belt_x +
magnet_belt_y +
magnet_belt_z +
gyros_arm_x +
gyros_arm_y +
gyros_arm_z +
accel_arm_x +
accel_arm_y +
accel_arm_z +
magnet_arm_x +
magnet_arm_y +
magnet_arm_z +
roll_dumbbell +
pitch_dumbbell +
yaw_dumbbell,
method="gbm",
trControl = fitControl, # an option for gbm
data=training,
verbose=FALSE)
gbmFit
# fit a boosted tree model via the gbm package --------------------------------
gbmFit <- train(classe ~ user_name +
pitch_arm +
yaw_arm +
roll_arm +
roll_belt +
pitch_belt +
yaw_belt +
gyros_belt_x +
gyros_belt_y +
gyros_belt_z +
accel_belt_x +
accel_belt_y +
accel_belt_z +
magnet_belt_x +
magnet_belt_y +
magnet_belt_z +
gyros_arm_x +
gyros_arm_y +
gyros_arm_z +
accel_arm_x +
accel_arm_y +
accel_arm_z +
magnet_arm_x +
magnet_arm_y +
magnet_arm_z +
roll_dumbbell +
pitch_dumbbell +
yaw_dumbbell,
method="gbm",
#trControl = fitControl, # an option for gbm
data=training,
verbose=FALSE)
gbmFit
# loading required libraries --------------------------------------------------
library(dplyr)
library(caret)
library(gbm)
install.packages('caret', dependencies = TRUE)
install.packages("caret", dependencies = TRUE)
setwd("~/Documents/CourseraMOOC/R_programming/machine-learning/practical-machine-learning-peer-review")
# loading required libraries --------------------------------------------------
library(dplyr)
library(caret)
library(gbm)
# reading in the data ---------------------------------------------------------
train <- read.csv("./data/pml-training.csv",
na.strings = "NA",
strip.white = TRUE)
train <- tbl_df(train)
eval <- read.csv("./data/pml-testing.csv",
na.strings = "NA",
strip.white = TRUE)
eval <- tbl_df(eval)
# splitting the training set into train and test ------------------------------
set.seed(123)
inTrain <- createDataPartition(y=train$classe, p=0.9, list=FALSE)
training <- train[inTrain,]
testing <- train[-inTrain,]
# fit a boosted tree model via the gbm package --------------------------------
gbmFit <- train(classe ~ user_name +
pitch_arm +
yaw_arm +
roll_arm +
roll_belt +
pitch_belt +
yaw_belt +
gyros_belt_x +
gyros_belt_y +
gyros_belt_z +
accel_belt_x +
accel_belt_y +
accel_belt_z +
magnet_belt_x +
magnet_belt_y +
magnet_belt_z +
gyros_arm_x +
gyros_arm_y +
gyros_arm_z +
accel_arm_x +
accel_arm_y +
accel_arm_z +
magnet_arm_x +
magnet_arm_y +
magnet_arm_z +
roll_dumbbell +
pitch_dumbbell +
yaw_dumbbell,
method="gbm",
trControl = fitControl, # an option for gbm
data=training,
verbose=FALSE)
gbmFit
# loading required libraries --------------------------------------------------
library(caret)
library(gbm)
library(dplyr)
# fit a boosted tree model via the gbm package --------------------------------
gbmFit <- train(classe ~ user_name +
pitch_arm +
yaw_arm +
roll_arm +
roll_belt +
pitch_belt +
yaw_belt +
gyros_belt_x +
gyros_belt_y +
gyros_belt_z +
accel_belt_x +
accel_belt_y +
accel_belt_z +
magnet_belt_x +
magnet_belt_y +
magnet_belt_z +
gyros_arm_x +
gyros_arm_y +
gyros_arm_z +
accel_arm_x +
accel_arm_y +
accel_arm_z +
magnet_arm_x +
magnet_arm_y +
magnet_arm_z +
roll_dumbbell +
pitch_dumbbell +
yaw_dumbbell,
method="gbm",
trControl = fitControl, # an option for gbm
data=training,
verbose=FALSE)
gbmFit
# fit a boosted tree model via the gbm package --------------------------------
gbmFit <- train(classe ~ user_name +
pitch_arm +
yaw_arm +
roll_arm +
roll_belt +
pitch_belt +
yaw_belt +
gyros_belt_x +
gyros_belt_y +
gyros_belt_z +
accel_belt_x +
accel_belt_y +
accel_belt_z +
magnet_belt_x +
magnet_belt_y +
magnet_belt_z +
gyros_arm_x +
gyros_arm_y +
gyros_arm_z +
accel_arm_x +
accel_arm_y +
accel_arm_z +
magnet_arm_x +
magnet_arm_y +
magnet_arm_z +
roll_dumbbell +
pitch_dumbbell +
yaw_dumbbell,
method="gbm",
#trControl = fitControl, # an option for gbm
data=training,
verbose=FALSE)
gbmFit
# predict the results using the training data ---------------------------------
predictTr <- predict(gbmFit,training)
table(predictTr, training$classe)
# predict the results using the training data ---------------------------------
predictTree <- predict(gbmFit,training)
table(predictTree, training$classe)
# predict the results using the training data ---------------------------------
predictTree <- predict(gbmFit,training)
table(predictTree, training$classe)
summary(gbmFit,n.trees=150)
# plot
qplot(roll_belt, yaw_belt,colour=classe,data=training)
# loading required libraries --------------------------------------------------
library(caret)
library(ggplot2)
library(gbm)
library(dplyr) # must be loaded after plyr in the caret package
#
ggplot(gbmFit)
predictTreeTest <- predict(gbmFit,testing)
table(predictTreeTest, testing$classe)
sum(diag(table(predictTreeTest, testing$classe)))
predictTreeTest <- predict(gbmFit,testing)
table(predictTreeTest, testing$classe)
sum(diag(table(predictTreeTest, testing$classe)))/sum(table(predictTreeTest, testing$classe))
predictTreeTest <- predict(gbmFit,testing)
tbl1 <- table(predictTreeTest, testing$classe)
tbl1
round(100*sum(diag(tbl1))/sum(tbl1),2)
#
predictTreeTest <- predict(gbmFit,testing)
tbl1 <- table(predictTreeTest, testing$classe)
tbl1
round(100*sum(diag(tbl1))/sum(tbl1),2)
#
predictTreeTest <- predict(gbmFit,testing)
tbl2 <- table(predictTreeTest, testing$classe)
tbl2
round(100*sum(diag(tbl2))/sum(tbl2),2)
# predict the results using the training data ---------------------------------
predictTree <- predict(gbmFit,training)
tbl1 <- table(predictTree, training$classe)
tbl1
summary(gbmFit,n.trees=150)
tbl1
round(100*sum(diag(tbl1))/sum(tbl1),2) # accuracy of the prediction
tbl2 <- xtabs(~predictTreeTest + testing$classe)
tbl2
#
predictTreeTest <- predict(gbmFit,testing)
#tbl2 <- table(predictTreeTest, testing$classe)
tbl2 <- xtabs(~predictTreeTest + testing$classe)
tbl2
round(100*sum(diag(tbl2))/sum(tbl2),2) # accuracy of the prediction
# plot
qplot(roll_belt, yaw_belt,colour=classe,data=training)
# predict the results using the training data ---------------------------------
predictTree <- predict(gbmFit,training)
tbl1 <- xtabs(~predictTree + training$classe)
tbl1
summary(gbmFit,n.trees=150)
tbl1
round(100*sum(diag(tbl1))/sum(tbl1),2) # accuracy of the prediction
#
predictTreeTest <- predict(gbmFit,testing)
tbl2 <- xtabs(~predictTreeTest + testing$classe)
tbl2
round(100*sum(diag(tbl2))/sum(tbl2),2) # accuracy of the prediction
#
predictTreeTest <- predict(gbmFit,testing)
tbl2 <- xtabs(~predictTreeTest + testing$classe)
tbl2
acc2 <- round(100*sum(diag(tbl2))/sum(tbl2),2) # accuracy of the prediction
acc2
# predict the results using the training data ---------------------------------
predictTree <- predict(gbmFit,training)
tbl1 <- xtabs(~predictTree + training$classe)
tbl1
summary(gbmFit,n.trees=150)
tbl1
acc1 <- round(100*sum(diag(tbl1))/sum(tbl1),2) # accuracy of the prediction
acc1
#
predictTreeTest <- predict(gbmFit,testing)
tbl2 <- xtabs(~predictTreeTest + testing$classe)
tbl2
acc2 <- round(100*sum(diag(tbl2))/sum(tbl2),2) # accuracy of the prediction
acc2
# plot
ggplot(gbmFit)
# plot
qplot(roll_belt, yaw_belt,colour=classe,data=training)
# plot
ggplot(gbmFit)
gbmFit
# plot separation using only the two most influential variables ---------------
qplot(roll_belt, yaw_belt,colour=classe,data=training)
# predict the results using the training data ---------------------------------
predictTree <- predict(gbmFit,training)
tbl1 <- xtabs(~predictTree + training$classe)
tbl1
summary(gbmFit,n.trees=150)
tbl1
acc1 <- round(100*sum(diag(tbl1))/sum(tbl1),2) # accuracy of the prediction
acc1
# plot separation using only the two most influential predictors --------------
qplot(roll_belt, yaw_belt,colour=classe,data=training)
# plot effectiveness of boosting and Tree Depth
ggplot(gbmFit)
# predict the results using the testing data ----------------------------------
predictTreeTest <- predict(gbmFit,testing)
tbl2 <- xtabs(~predictTreeTest + testing$classe)
tbl2
acc2 <- round(100*sum(diag(tbl2))/sum(tbl2),2) # accuracy of the prediction
acc2
confusionMatrix(predictTreeTest, testing$classe)
confusionMatrix(predictTree, training$classe)
# predict the results using the training data ---------------------------------
predictTree <- predict(gbmFit,training)
tbl1 <- xtabs(~predictTree + training$classe)
tbl1
summary(gbmFit,n.trees=150)
tbl1
acc1 <- round(100*sum(diag(tbl1))/sum(tbl1),2) # accuracy of the prediction
acc1
confusionMatrix(predictTree, training$classe)
View(eval)
# predict the results using the evaluation data ----------------------------------
predictTreeEval <- predict(gbmFit,eval)
tbl3 <- xtabs(~predictTreeEval + eval$classe)
tbl3
acc3 <- round(100*sum(diag(tbl3))/sum(tbl3),2) # accuracy of the prediction
acc3
confusionMatrix(predictTreeEval, eval$classe)
View(testing)
predictTreeEval <- predict(gbmFit,eval)
testing$classe
eval$classe
training$classe
train$classe
names(eval)
names(testing)
# predict the results using the evaluation data ----------------------------------
predictTreeEval <- predict(gbmFit,eval)
tbl3 <- xtabs(~predictTreeEval + eval$problem_id)
tbl3
acc3 <- round(100*sum(diag(tbl3))/sum(tbl3),2) # accuracy of the prediction
acc3
confusionMatrix(predictTreeEval, eval$problem_id)
tbl3 <- xtabs(~predictTreeEval + eval$problem_id)
tbl3
acc3 <- round(100*sum(diag(tbl3))/sum(tbl3),2) # accuracy of the prediction
acc3
confusionMatrix(predictTreeEval, eval$problem_id)
view(testing$classe)
View(testing$classe)
View(eval$problem_id)
View(training$classe)
View(select(classe, training))
View(training[, "classe"])
View(eval[, "classe"])
confusionMatrix(predictTreeEval)
predictTreeEval <- predict(gbmFit,eval)
confusionMatrix(predictTreeEval, eval)
predictTreeEval_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
predictTreeEval_files(predictTreeEval)
tbl3 <- xtabs(~predictTreeEval + eval$problem_id)
tbl3
predictTreeEval_files(predictTreeEval)
eval$problem_id
predictTreeEval <- predict(gbmFit,eval)answer<-predict(gbmFit,eval)
answer
predictTreeEval <- predict(gbmFit,eval)answer<-predict(gbmFit,eval)
answer<-predict(gbmFit,eval)
answer
# predict the results using the evaluation data ----------------------------------
answer<-predict(gbmFit,eval)
answer
# code as suggested by Coursera
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answer)
# predict the results using the training data ---------------------------------
predictTree <- predict(gbmFit,training)
tbl1 <- xtabs(~predictTree + training$classe)
tbl1
summary(gbmFit,n.trees=150)
tbl1
acc1 <- round(100*sum(diag(tbl1))/sum(tbl1),2) # accuracy of the prediction
acc1
confusionMatrix(predictTree, training$classe)
